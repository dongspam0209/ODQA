{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import nltk\n",
    "#nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "dataset = load_from_disk(\"./resources/data/train_dataset/\")\n",
    "train_dataset = dataset['train']\n",
    "val_dataset = dataset['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ephemeral/home/mrc-nlp-08/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, EvalPrediction, TrainingArguments, DataCollatorWithPadding, EarlyStoppingCallback, AutoModelForSeq2SeqLM\n",
    "from datasets import load_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, BartForConditionalGeneration\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gogamza/kobart-base-v2\")\n",
    "model = BartForConditionalGeneration.from_pretrained(\"gogamza/kobart-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'ë¯¸êµ­ ìƒì›',\n",
       " 'context': 'ë¯¸êµ­ ìƒì˜ì› ë˜ëŠ” ë¯¸êµ­ ìƒì›(United States Senate)ì€ ì–‘ì›ì œì¸ ë¯¸êµ­ ì˜íšŒì˜ ìƒì›ì´ë‹¤.\\\\n\\\\në¯¸êµ­ ë¶€í†µë ¹ì´ ìƒì›ì˜ì¥ì´ ëœë‹¤. ê° ì£¼ë‹¹ 2ëª…ì˜ ìƒì›ì˜ì›ì´ ì„ ì¶œë˜ì–´ 100ëª…ì˜ ìƒì›ì˜ì›ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆë‹¤. ì„ê¸°ëŠ” 6ë…„ì´ë©°, 2ë…„ë§ˆë‹¤ 50ê°œì£¼ ì¤‘ 1/3ì”© ìƒì›ì˜ì›ì„ ìƒˆë¡œ ì„ ì¶œí•˜ì—¬ ì—°ë°©ì— ë³´ë‚¸ë‹¤.\\\\n\\\\në¯¸êµ­ ìƒì›ì€ ë¯¸êµ­ í•˜ì›ê³¼ëŠ” ë‹¤ë¥´ê²Œ ë¯¸êµ­ ëŒ€í†µë ¹ì„ ìˆ˜ë°˜ìœ¼ë¡œ í•˜ëŠ” ë¯¸êµ­ ì—°ë°© í–‰ì •ë¶€ì— ê°ì¢… ë™ì˜ë¥¼ í•˜ëŠ” ê¸°ê´€ì´ë‹¤. í•˜ì›ì´ ì„¸ê¸ˆê³¼ ê²½ì œì— ëŒ€í•œ ê¶Œí•œ, ëŒ€í†µë ¹ì„ í¬í•¨í•œ ëŒ€ë‹¤ìˆ˜ì˜ ê³µë¬´ì›ì„ íŒŒë©´í•  ê¶Œí•œì„ ê°–ê³  ìˆëŠ” êµ­ë¯¼ì„ ëŒ€í‘œí•˜ëŠ” ê¸°ê´€ì¸ ë°˜ë©´ ìƒì›ì€ ë¯¸êµ­ì˜ ì£¼ë¥¼ ëŒ€í‘œí•œë‹¤. ì¦‰ ìº˜ë¦¬í¬ë‹ˆì•„ì£¼, ì¼ë¦¬ë…¸ì´ì£¼ ê°™ì´ ì£¼ ì •ë¶€ì™€ ì£¼ ì˜íšŒë¥¼ ëŒ€í‘œí•˜ëŠ” ê¸°ê´€ì´ë‹¤. ê·¸ë¡œ ì¸í•˜ì—¬ êµ°ëŒ€ì˜ íŒŒë³‘, ê´€ë£Œì˜ ì„ëª…ì— ëŒ€í•œ ë™ì˜, ì™¸êµ­ ì¡°ì•½ì— ëŒ€í•œ ìŠ¹ì¸ ë“± ì‹ ì†ì„ ìš”í•˜ëŠ” ê¶Œí•œì€ ëª¨ë‘ ìƒì›ì—ê²Œë§Œ ìˆë‹¤. ê·¸ë¦¬ê³  í•˜ì›ì— ëŒ€í•œ ê²¬ì œ ì—­í• (í•˜ì›ì˜ ë²•ì•ˆì„ ê±°ë¶€í•  ê¶Œí•œ ë“±)ì„ ë‹´ë‹¹í•œë‹¤. 2ë…„ì˜ ì„ê¸°ë¡œ ì¸í•˜ì—¬ ê¸‰ì§„ì ì¼ ìˆ˜ë°–ì— ì—†ëŠ” í•˜ì›ì€ ì§€ë‚˜ì¹˜ê²Œ ê¸‰ì§„ì ì¸ ë²•ì•ˆì„ ë§Œë“¤ê¸° ì‰½ë‹¤. ëŒ€í‘œì ì¸ ì˜ˆë¡œ ê±´ê°•ë³´í—˜ ê°œí˜ ë‹¹ì‹œ í•˜ì›ì´ ë¯¸êµ­ ì—°ë°© í–‰ì •ë¶€ì—ê²Œ í¼ë¸”ë¦­ ì˜µì…˜(ê³µê³µê±´ê°•ë³´í—˜ê¸°ê´€)ì˜ ì¡°í•­ì´ ìˆëŠ” ë°˜ë©´ ìƒì›ì˜ ê²½ìš° í•˜ì›ì•ˆì´ ì§€ë‚˜ì¹˜ê²Œ ì„¸ê¸ˆì´ ë§ì´ ë“ ë‹¤ëŠ” ì´ìœ ë¡œ í¼ë¸”ë¦­ ì˜µì…˜ ì¡°í•­ì„ ì œì™¸í•˜ê³  ë¹„ì˜ë¦¬ê±´ê°•ë³´í—˜ê¸°ê´€ì´ë‚˜ ë³´í—˜íšŒì‚¬ê°€ ë‹´ë‹¹í•˜ë„ë¡ í•œ ê²ƒì´ë‹¤. ì´ ê²½ìš°ì²˜ëŸ¼ ìƒì›ì€ í•˜ì›ì´ë‚˜ ë‚´ê°ì±…ì„ì œê°€ ë¹ ì§€ê¸° ì‰¬ìš´ êµ­ê°€ë“¤ì˜ êµ­íšŒì²˜ëŸ¼ ê±¸í•í•˜ë©´ ë°œìƒí•˜ëŠ” ì˜íšŒì˜ ë¹„ì •ìƒì ì¸ ì‚¬íƒœë¥¼ ë°©ì§€í•˜ëŠ” ê¸°ê´€ì´ë‹¤. ìƒì›ì€ ê¸‰ë°•í•œ ì²˜ë¦¬ì‚¬í•­ì˜ ê²½ìš°ê°€ ì•„ë‹ˆë©´ ë²•ì•ˆì„ ë¨¼ì € ë‚´ëŠ” ê²½ìš°ê°€ ë“œë¬¼ê³  í•˜ì›ì´ ë§Œë“  ë²•ì•ˆì„ ìˆ˜ì •í•˜ì—¬ ë‹¤ì‹œ í•˜ì›ì— ë˜ëŒë ¤ë³´ë‚¸ë‹¤. ì´ëŸ¬í•œ ë°©ì‹ìœ¼ë¡œ ë‹¨ì›ì œê°€ ë¹ ì§€ê¸° ì‰¬ìš´ í•¨ì •ì„ ë¯¸ë¦¬ ë°©ì§€í•˜ëŠ” ê²ƒì´ë‹¤.ë‚ ì§œ=2017-02-05',\n",
       " 'question': 'ëŒ€í†µë ¹ì„ í¬í•¨í•œ ë¯¸êµ­ì˜ í–‰ì •ë¶€ ê²¬ì œê¶Œì„ ê°–ëŠ” êµ­ê°€ ê¸°ê´€ì€?',\n",
       " 'id': 'mrc-1-000067',\n",
       " 'answers': {'answer_start': [235], 'text': ['í•˜ì›']},\n",
       " 'document_id': 18293,\n",
       " '__index_level_0__': 42}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3952, 240)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3952/3952 [00:02<00:00, 1501.05 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 240/240 [00:00<00:00, 1507.18 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Train preprocessing / ì „ì²˜ë¦¬ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤.\n",
    "def prepare_features(examples):\n",
    "    # truncationê³¼ padding(lengthê°€ ì§§ì„ë•Œë§Œ)ì„ í†µí•´ toknizationì„ ì§„í–‰í•˜ë©°, strideë¥¼ ì´ìš©í•˜ì—¬ overflowë¥¼ ìœ ì§€í•©ë‹ˆë‹¤.\n",
    "    # ê° exampleë“¤ì€ ì´ì „ì˜ contextì™€ ì¡°ê¸ˆì”© ê²¹ì¹˜ê²Œë©ë‹ˆë‹¤.\n",
    "    labels = [answers['text'][0] for answers in examples['answers']]\n",
    "    tokenized_examples = tokenizer(\n",
    "        text=examples['question'],\n",
    "        text_pair=examples['context'],\n",
    "        text_target=labels,\n",
    "        padding=\"max_length\",\n",
    "        truncation=\"only_second\",    # contextë§Œ truncateí•˜ê¸°\n",
    "        max_length=512,\n",
    "        stride=128, # ì´ì „ chunkì™€ overlapë˜ëŠ” ë¶€ë¶„ì„ ë‘ì–´ ê¸´ ë¬¸ì„œë¥¼ ì²˜ë¦¬í•  ë•Œ ìœ ìš©. ëª¨ë¸ì´ ë” ë‚˜ì€ ì„ë² ë”©ì„ í•˜ë„ë¡ ë„ì™€ QAì— ìœ ìš©.\n",
    "        return_overflowing_tokens=True,\n",
    "    )\n",
    "    del tokenized_examples['token_type_ids']\n",
    "    # labels í™•ì¥\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "    tokenized_examples[\"labels\"] = [tokenized_examples['labels'][i] for i in sample_mapping]\n",
    "    \n",
    "    return tokenized_examples\n",
    "\n",
    "train_dataset = train_dataset.map(\n",
    "            prepare_features,\n",
    "            batched=True,\n",
    "            num_proc=None,\n",
    "            remove_columns=train_dataset.column_names,\n",
    "            load_from_cache_file=not False)\n",
    "\n",
    "val_dataset = val_dataset.map(\n",
    "            prepare_features,\n",
    "            batched=True,\n",
    "            num_proc=None,\n",
    "            remove_columns=val_dataset.column_names,\n",
    "            load_from_cache_file=not False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['input_ids', 'attention_mask', 'labels'],\n",
       "     num_rows: 5107\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['input_ids', 'attention_mask', 'labels'],\n",
       "     num_rows: 307\n",
       " }))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ephemeral/home/mrc-nlp-08/.venv/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "metric = load_metric(\"squad\")\n",
    "\n",
    "def compute_metrics(p: EvalPrediction): #EvalPrediction êµ¬ì¡° | predictions: ëª¨ë¸ì˜ ì˜ˆì¸¡ê°’, label_ids: ì‹¤ì œ ì •ë‹µ ë ˆì´ë¸”\n",
    "    predictions = p.predictions\n",
    "    references = p.label_ids\n",
    "    \n",
    "    if np.array(predictions).ndim == 3:\n",
    "        predictions = np.argmax(predictions, axis=-1)\n",
    "        \n",
    "    predictions = [{'prediction_text': pred, 'id': str(i)} for i, pred in enumerate(predictions)]\n",
    "    references = [{'answers': {'answer_start': [], 'text': [label]}, 'id': str(i)} for i, label in enumerate(references)]\n",
    "\n",
    "    result = metric.compute(predictions=predictions, references=references)\n",
    "    result['eval_exact_match'] = result['exact_match']\n",
    "    del result['exact_match']\n",
    "    result['eval_f1'] = result['f1']\n",
    "    del result['f1']\n",
    "    return result\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./ckpt/bart\",num_train_epochs = 25,\n",
    "    save_strategy='epoch', evaluation_strategy=\"epoch\",\n",
    "    remove_unused_columns=False, optim=\"adamw_hf\",\n",
    "    per_device_train_batch_size=16, weight_decay=0.01,\n",
    "    save_total_limit = 1, load_best_model_at_end = True)\n",
    "\n",
    "    #warmup_steps=1000,\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8 if training_args.fp16 else None)\n",
    "\n",
    "# Trainer ì´ˆê¸°í™”\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    "    #callbacks = [EarlyStoppingCallback(early_stopping_patience=10)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ephemeral/home/mrc-nlp-08/.venv/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2719' max='8000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2719/8000 50:41 < 1:38:32, 0.89 it/s, Epoch 8.49/25]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.013695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.007500</td>\n",
       "      <td>0.014684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.007500</td>\n",
       "      <td>0.015471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>0.016835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.016778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.016847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.017352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.018456</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mrc-nlp-08/.venv/lib/python3.10/site-packages/transformers/trainer.py:2052\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2050\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2051\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2052\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2053\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2054\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2055\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2056\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2057\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mrc-nlp-08/.venv/lib/python3.10/site-packages/transformers/trainer.py:2393\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2387\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m   2388\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   2390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2391\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2392\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m-> 2393\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   2394\u001b[0m ):\n\u001b[1;32m   2395\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2396\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[1;32m   2397\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(resume_from_checkpoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(context, question):\n",
    "    # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •\n",
    "    trainer.model.eval()\n",
    "\n",
    "\n",
    "    # ì…ë ¥ í…ìŠ¤íŠ¸ë¥¼ í† í¬ë‚˜ì´ì¦ˆ\n",
    "    inputs = tokenizer(\n",
    "            text=question,\n",
    "            text_pair=context,\n",
    "            padding=\"max_length\",\n",
    "            truncation=\"only_second\",    # contextë§Œ truncateí•˜ê¸°\n",
    "            max_length=384,\n",
    "            stride=128, # ì´ì „ chunkì™€ overlapë˜ëŠ” ë¶€ë¶„ì„ ë‘ì–´ ê¸´ ë¬¸ì„œë¥¼ ì²˜ë¦¬í•  ë•Œ ìœ ìš©. ëª¨ë¸ì´ ë” ë‚˜ì€ ì„ë² ë”©ì„ í•˜ë„ë¡ ë„ì™€ QAì— ìœ ìš©.\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "    del inputs['token_type_ids']\n",
    "\n",
    "    # GPU ì‚¬ìš© ì‹œ ì…ë ¥ì„ GPUë¡œ ì´ë™\n",
    "    inputs = {k: torch.tensor(v).to(trainer.model.device) for k, v in inputs.items()}\n",
    "\n",
    "    outputs = model.generate(**inputs, max_length=30, num_beams=5)\n",
    "    '''# inference ìˆ˜í–‰\n",
    "    with torch.no_grad():\n",
    "        outputs = trainer.model(**inputs)'''\n",
    "    # ê²°ê³¼ ì²˜ë¦¬\n",
    "    #predictions = outputs.argmax(-1)\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: 'á„á„á„'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_474890/3863879182.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs = {k: torch.tensor(v).to(trainer.model.device) for k, v in inputs.items()}\n"
     ]
    }
   ],
   "source": [
    "context = ''''ã…‹ã…‹'ì€ ì¼ë°˜ì ìœ¼ë¡œ ì›ƒìŒì†Œë¦¬ë¥¼ í‘œí˜„í•  ë•Œ ì‚¬ìš©ëœë‹¤. í†µì‹ ì²´ì—ì„œëŠ” ì›ƒëŠ” ëª¨ìŠµì„ ì§ì ‘ì ìœ¼ë¡œ ë³´ì—¬ì¤„ ìˆ˜ ì—†ê¸° ë•Œë¬¸ì— ì˜ì„±ì–´ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš°ê°€ ë§ì€ë° ê·¸ ì¤‘ì—ì„œ 'í‚¥í‚¥', 'í­í­'ì„ ì´ˆì„±ì²´ë¡œ ë°”ê¾¸ì–´ ì‚¬ìš©í•œ ê²ƒì´ 'ã…‹ã…‹'ì— í•´ë‹¹í•œë‹¤. ì´ì— ëŒ€í•´ 2001ë…„ ì—°êµ¬ì—ì„œëŠ” \"ììŒë§Œ ê°€ì§€ê³  í‘œì‹œí•˜ì—¬ í†µì‹ ìƒì˜ ì¬ë¯¸ë¥¼ ë”í•˜ê¸° ìœ„í•œ ë°©ë²•ìœ¼ë¡œ ì‚¬ìš©ë˜ê³  ìˆë‹¤\"ê³  í•´ì„í•œ ë°” ìˆë‹¤. \\n\\nã…‹ìë¥¼ ì›ƒìŒì†Œë¦¬ë¥¼ ì˜ë¯¸í•˜ëŠ” ê²ƒìœ¼ë¡œ ì‚¬ìš©í•  ë•Œ, 'ã…‹'ì í•œ ê°œì—ì„œ 'ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹...'í•˜ëŠ” ì‹ìœ¼ë¡œ ì—´ íƒ€ ì´ìƒ í•œ ì¤„ ë„˜ê²Œê¹Œì§€ ì“°ì´ê¸°ë„ í•œë‹¤ ã…‹ìë¥¼ ì–¼ë§ˆë‚˜ ë§ì´ ì¼ëŠëƒì— ë”°ë¼ ê·¸ ëŠë‚Œê³¼ ì˜ë¯¸ëŠ” ë‹¬ë¼ì§„ë‹¤. ì‹ ì¡°ì–´ë¥¼ ì „ë¬¸ìœ¼ë¡œ ì—°êµ¬í•˜ëŠ” ì´ì¬í˜„ ë¬¸í™”í‰ë¡ ê°€ëŠ” ã€Ší•œêµ­ì¼ë³´ã€‹ì˜ ê¸°ê³ ê¸€ì—ì„œ ê·¸ ì¢…ë¥˜ì™€ ëŠë‚Œì´ ëŒ€ëµ ë‹¤ìŒê³¼ ê°™ë‹¤ê³  ë³´ì•˜ë‹¤\\n* ã…‹ - ì¼ë°˜ì ìœ¼ë¡œ ë¬´ì‹¬í•˜ê²Œ ë™ì˜í•˜ë ¤ëŠ” ìƒí™©ì—ì„œ ì“°ì¸ë‹¤. ë‹¤ë¥¸ ë§ëì— ë¶™ì˜€ì„ ë•Œ, ì´ë¥¼í…Œë©´ 'ê·¸ë˜ã…‹', 'ì¢‹ë„¤ã…‹', 'ì˜¬ã…‹' ë“±ì€ ì›ƒìŒ ìì²´ë¼ê¸°ë³´ë‹¤ëŠ” ì…ê¼¬ë¦¬ê°€ ì•„ì£¼ ì‚´ì§ ì˜¬ë¼ê°„ ìƒí™©ì´ë¼ëŠ” ëŠë‚Œì„ ì¤€ë‹¤. ì˜ì„±ì–´ë¡œëŠ” \"í­\" í˜¹ì€ \"í‚¥\"ìœ¼ë¡œ í•´ì„ë  ìˆ˜ ìˆë‹¤. í•œí¸ 'ë­í•´?ã…‹', 'ë¯¸ì•ˆã…‹'ì˜ ê²½ìš° 'ã…‹'ëŠ” ìƒíˆ¬ì ì¸ êµ°ë§ì˜ ì—­í• ì„ í•œë‹¤.\\n* ã…‹ã…‹ - ìœ„ì˜ ê²ƒë³´ë‹¤ ã…‹ìê°€ í•˜ë‚˜ê°€ ë” ë§ìœ¼ë‚˜ ì˜¤íˆë ¤ í˜•ì‹ì ì´ë¼ëŠ” ëŠë‚Œì„ ì¤€ë‹¤. ì¢‹ê²ŒëŠ” 'ê·¸ë ‡êµ°'ì„ ëœ»í•˜ê±°ë‚˜, ì¡°ê¸ˆ ë‚˜ì˜ê²ŒëŠ” ëŒ€í™”ìƒì—ì„œ ì˜ë¡€ì ìœ¼ë¡œ ì¶”ì„ìƒˆë¥¼ ë„£ê³  ìˆë‹¤ëŠ” ëŠë‚Œì„ ì¤€ë‹¤. 'ã…‹ã…‹'ì€ ë¯¸í˜¼ë‚¨ë…€ë“¤ì´ ê°€ì¥ ì‹«ì–´í•˜ëŠ” ì„±ì˜ ì—†ëŠ” ë©”ì‹ ì € ë§íˆ¬ë¡œ ì„ ì •ë˜ê¸°ë„ í–ˆë‹¤. ë‹¤ë§Œ íšŒì‚¬ ìƒì‚¬ì—ê²Œ ëŒ€ë‹µí•˜ëŠ” ê²½ìš° ê°™ì€ ê³µì ì¸ ìƒí™©ì—ì„œëŠ” 'ë„¤ã…‹ã…‹'ì™€ ê°™ì€ ëŒ€ë‹µì€ ê°€ë²¼ì›Œ ë³´ì¸ë‹¤ëŠ” ì¡°ì‚¬ë„ ìˆì—ˆë‹¤. \\n* ã…‹ã…‹ã…‹ - ë¹„êµì  ì¤‘ë¦½ì ì´ë©´ì„œë„ ê°€ì¥ ë§ì´ ì‚¬ìš©ë˜ëŠ” ìš©ë²•ì´ë‹¤. ì›ƒëŠ” ê°ì •ì„ ê·¸ëŒ€ë¡œ í‘œí˜„í•œë‹¤ëŠ” ë‰˜ì•™ìŠ¤ë¥¼ ê°–ëŠ”ë‹¤.\\n* ã…‹ã…‹ã…‹ã…‹... - ì—¬ê¸°ì„œë¶€í„°ëŠ” \"ì •ë§ ì›ƒê¸´ë‹¤\"ë¼ëŠ” ë°˜ì‘ì„ ë‚˜ë¦„ ì •ì„±ë“¤ì—¬ì„œ í‘œí˜„í•˜ëŠ” ëŠë‚Œì„ ì¤€ë‹¤. ë„¤ ê°œë¶€í„° ê·¸ ì´ìƒì€ ì˜ë¯¸ì˜ í•¨ì¶•ê³¼ ì •ì„œì˜ ê°•ë„ê°€ ê±°ì˜ ê°™ìœ¼ë©°, ì´ëŸ° ì ì—ì„œ 'ã…‹ã…‹ã…‹ã…‹'ëŠ” ì‹¤ì§ˆì ìœ¼ë¡œ ì›ƒê¸´ë‹¤ëŠ” ê²ƒì„ í‘œí˜„í•˜ëŠ” ìµœì†Œ ì¡°ê±´ì— í•´ë‹¹í•œë‹¤.'''\n",
    "question = 'ì›ƒëŠ” ê°ì •ì„ ê·¸ëŒ€ë¡œ í‘œí˜„í•œë‹¤ëŠ” ë‰˜ì•™ìŠ¤ë¥¼ ê°–ëŠ” ã…‹ì˜ ìš©ë²•ì€?'\n",
    "generated_text = inference(context, question)\n",
    "print(f\"Generated text: {generated_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: ë™ê³„ê±´ì¡°\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_474890/3863879182.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs = {k: torch.tensor(v).to(trainer.model.device) for k, v in inputs.items()}\n"
     ]
    }
   ],
   "source": [
    "context = '''ëƒ‰ëŒ€ ë™ê³„ ì†Œìš° ê¸°í›„(å†·å¸¦å†¬å­£å°‘é›¨æ°£å€™)ëŠ” ì¾¨íœì˜ ê¸°í›„ êµ¬ë¶„ì—ì„œ ëƒ‰ëŒ€ ê¸°í›„ì— ì†í•˜ë©°, ê¸°í˜¸ëŠ” Dwa, Dwb, Dwc, Dwdë¡œ, DëŠ” ëƒ‰ëŒ€, wëŠ” ë™ê³„ê±´ì¡°(wintertrocken)ì„ ê°€ë¦¬í‚¨ë‹¤. íŠ¸ë€ìŠ¤ ë°”ì´ì¹¼ ê¸°í›„ ë¼ê³ ë„ í•œë‹¤. ì—¬ë¦„ì—ëŠ” ì˜¨ë„ê°€ ë¹„êµì  ë†’ê³ , ê²¨ìš¸ì—ëŠ” ë§‘ì€ ë‚ ì”¨ê°€ ì§€ì†ë˜ê³  ì•½í•œ ë°”ëŒì´ ë¶„ë‹¤. ë³µì‚¬ëƒ‰ê° ë•Œë¬¸ì— ë§¤ìš° ìŒ€ìŒ€í•œ ê²ƒì´ íŠ¹ì§•ì´ë‹¤. ë˜ ì—¬ë¦„ì—ëŠ” ê³„ì ˆí’ì˜ ì˜í–¥ìœ¼ë¡œ ë¹„ê°€ ë§ì´ ë‚´ë¦°ë‹¤. ê²¨ìš¸ì€ ì—¬ë¦„ì— ë¹„í•´ ë§¤ìš° ê¸´ í¸ì´ë‹¤. ê¸°ì˜¨ì˜ ì—°êµì°¨ê°€ ë§¤ìš° í¬ê²Œ ë‚˜íƒ€ë‚˜ëŠ” ëŒ€ë¥™ì„± ê¸°í›„ê°€ ì§™ê²Œ ë‚˜íƒ€ë‚œë‹¤.\\n\\nëƒ‰ëŒ€ ë™ê³„ ì†Œìš° ê¸°í›„ëŠ” ë§¤ìš° ì¶”ìš´ ê¸°í›„ë¡œ, ë†ì‘ë¬¼ì„ ì¬ë°°í•  ìˆ˜ ìˆëŠ” ê³³ê³¼ ì¬ë°° ìˆ˜ì¢… ë° ìˆ˜ê°€ ì œí•œë˜ì–´ ìˆë‹¤. ìˆ˜ìˆ˜, ì˜¥ìˆ˜ìˆ˜, ë´„ë°€ ë“±ì„ ì¬ë°°í•œë‹¤. ë¶ë¶€ëŠ” íƒ€ì´ê°€ ê¸°í›„ì™€ ê°™ì´ íƒ€ì´ê°€ê°€ ë§ì´ ë‚˜íƒ€ë‚˜ ì„ì—…í™œë™ì„ í•œë‹¤. ìˆ² ë˜í•œ ëƒ‰ëŒ€ ìŠµìœ¤ ê¸°í›„ì™€ ë¹„ìŠ·í•˜ì—¬ ë‚¨ë¶€ì—ëŠ” ì¹¨ì—½ìˆ˜ì™€ í™œì—½ìˆ˜ê°€ í•¨ê»˜ ë‚˜íƒ€ë‚˜ëŠ” í˜¼í•©ë¦¼ì´, ë¶ë¶€ì—ëŠ” íƒ€ì´ê°€(ì¹¨ì—½ìˆ˜ë¦¼)ì´ ì˜ ë‚˜íƒ€ë‚œë‹¤. ë¶ë¶€ëŠ” ì¶”ìœ„ì—ë„ ì˜ ê²¬ë””ëŠ” ê°€ëŠ˜ê³  ë¾°ì¡±í•œ ìì„ ê°€ì§„ ì¹¨ì—½ìˆ˜ê°€ ìë¼ê¸° ìœ ë¦¬í•˜ê¸° ë•Œë¬¸ì— íƒ€ì´ê°€ê°€, ë‚¨ë¶€ì—ëŠ” ë¶ë¶€ë³´ë‹¤ ë”°ëœ»í•˜ì—¬ í˜¼í•©ë¦¼ì´ ì˜ ë‚˜íƒ€ë‚œë‹¤. ë‚¨ë¶€ ì§€ì—­ì˜ ê·¹ì†Œìˆ˜ì—ì„œë§Œ ë²¼ ì¬ë°°ê°€ ê°€ëŠ¥í•˜ë‹¤.\\n\\ní† ì–‘ì€ ì£¼ë¡œ í¬ë“œì¡¸ì´ ë‚˜íƒ€ë‚˜ê³ , í˜¼í•©ë¦¼ì—ì„œëŠ” ê°ˆìƒ‰ ì‚¼ë¦¼í† ê°€ ë‚˜íƒ€ë‚˜ëŠ”ë°, í¬ë“œì¡¸ì€ ìœ ê¸°ë¬¼ì´ ë¶„í•´ë˜ì§€ëŠ” ì•Šì•˜ìœ¼ë‚˜, ì†Œê¸ˆê¸°ë¥¼ ì˜ ë¨¸ê¸ˆì§€ ì•Šê³  ì¹¨ì—½ìˆ˜ë¦¼ì˜ íŠ¹ì„± ë•Œë¬¸ì— ë†ì‘ë¬¼ì„ ì¬ë°°í•˜ëŠ” ë°ì—ëŠ” ì í•©í•˜ì§€ ì•Šë‹¤. ê·¸ì— ë°˜í•´ ê°ˆìƒ‰ ì‚¼ë¦¼í† ëŠ” ë¬¼ì„ ì˜ í¡ìˆ˜í•˜ê³  í¬ë“œì¡¸ì— ë¹„í•´ ì•½ì‚°ì„±ì„ ë ê¸° ë•Œë¬¸ì— ë†ì‘ë¬¼ ì¬ë°°ì™€ ëª©ì¶•ì— ì•Œë§ë‹¤.\\n\\nì´ëŸ° ëƒ‰ëŒ€ ë™ê³„ ì†Œìš° ê¸°í›„ê°€ ë‚˜íƒ€ë‚˜ëŠ” ê³³ìœ¼ë¡œëŠ” ì‹œë² ë¦¬ì•„ ë‚´ë¥™ ë™ë¶€, ì¤‘êµ­ í™”ë² ì´ ë™ë¶€ ë° ë‘¥ë² ì´, ëŒ€í•œë¯¼êµ­ ê°•ì›ë„ ë‚´ë¥™ ì§€ì—­ê³¼ ê²½ê¸°ë„ ë° ì‚°ì•…ì§€ëŒ€, ì¡°ì„ ë¯¼ì£¼ì£¼ì˜ì¸ë¯¼ê³µí™”êµ­ ëŒ€ë¶€ë¶„, ìºë‚˜ë‹¤ ë° ë¯¸êµ­, ë„¤íŒ”ì˜ ì¼ë¶€ ì§€ì—­ ë“±ì´ ìˆìœ¼ë©°, íŠ¹íˆ ì‹œë² ë¦¬ì•„ ë™ë¶€ëŠ” ì„¸ê³„ì˜ í•œê·¹ì„ ë§Œë“¤ì–´ ë‚¸ë‹¤.\\n\\nê·¸ë˜ì„œ ì´ ê¸°í›„ê°€ ëƒ‰ëŒ€ ê¸°í›„ì˜ íƒˆì„ ì“´ ì‚¬ë°”ë‚˜ ê¸°í›„ë¼ê³  ë´ì•¼ í•˜ëŠ” ìˆ˜ì¤€ì— ì´ë¥´ê²Œ ë˜ë©°, ê²¨ìš¸ì² ì—ëŠ” ê°•ìˆ˜ëŸ‰ì´ ì ì–´ì ¸ì„œ ê·¸ë ‡ê²Œ ê±´ì¡°í•œ ë§Œí¼ ë¶ˆì”¨ê°€ ì‰½ê²Œ ì¼ì–´ë‚˜ëŠ” íŠ¹ì„±ìƒ ì‚°ë¶ˆë„ ë¬¼ë¡  ì¼ì–´ë‚˜ê¸°ê°€ ì‰¬ìš´ ê¸°í›„ ëŒ€ì—­ì— ì†í•œë‹¤.\\n\\nì•ìœ¼ë¡œ ì˜¨ë‚œí™” ì—¬íŒŒì— ë”°ë¼ ì´ ê¸°í›„ê°€ ì‚¬ë¼ì§ˆ ê°€ëŠ¥ì„±ì´ ìˆëŠ” ì§€ì—­ë„ ìƒê¸¸ ìˆ˜ë„ ìˆë‹¤. ë‹¨ì ì¸ ì˜ˆë¡œ ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ê¶Œ, ê°•ì›ë„ ë“±ì€ í˜„ì¬ Dwaë¡œ ë¶„ë¥˜ë˜ë‚˜ ì˜¨ëŒ€ ê¸°í›„ë¡œ ë°”ë€” ê°€ëŠ¥ì„±ì´ ìˆë‹¤.'''\n",
    "question = 'ëƒ‰ëŒ€ ë™ê³„ ì†Œìš° ê¸°í›„ì˜ wëŠ” ë¬´ì—‡ì˜ ì•½ìì¸ê°€?'\n",
    "generated_text = inference(context, question)\n",
    "print(f\"Generated text: {generated_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: ã€Šì‚¬ì´ì¢‹ì€ ê°€ìš°ìŠ¤ì „ìã€‹\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_474890/3863879182.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs = {k: torch.tensor(v).to(trainer.model.device) for k, v in inputs.items()}\n"
     ]
    }
   ],
   "source": [
    "context = '''í•œêµ­ì˜ ì›¹íˆ°.\\n\\nê³½ë°±ìˆ˜ê°€ ë„¤ì´ë²„ì—ì„œ ì—°ì¬í•œ ì›¹íˆ°. íšŒì‚¬ ìƒí™œì˜ ì´ì•¼ê¸°ë¥¼ ì£¼ì¶•ìœ¼ë¡œ í•˜ëŠ” ì§ì¥ ë§Œí™”ë¡œ, ì‘ê°€ì˜ ì´ì „ í¥í–‰ì‘ì¸ã€ŠíŠ¸ë¼ìš°ë§ˆã€‹ì—ì„œë„ ìµíˆ ì´ë¦„ì„ ì•Œë ¸ë˜ ê°€ìš°ìŠ¤ ê·¸ë£¹#s-1.3ì´ ì•„ì˜ˆ ë©”ì¸ìœ¼ë¡œ ë‹¤ë¤„ì§„ë‹¤. ë”°ë¼ì„œ, ì–´ë–¤ ì˜ë¯¸ì—ì„œëŠ” íŠ¸ë¼ìš°ë§ˆì˜ ìŠ¤í•€ì˜¤í”„ì‘ì´ë¼ê³ ë„ ë³¼ìˆ˜ ìˆëŠ” ì…ˆ. ë¬¼ë¡ , ê°€ìš°ìŠ¤ ê·¸ë£¹ì˜ ë§‰ë‚˜ê°€ëŠ” ì„¼ìŠ¤ëŠ” ì—¬ì „í•˜ë‹¤. ì´ˆê¸°ì—ëŠ” ã€Šì‚¬ì´ì¢‹ì€ ê°€ìš°ìŠ¤ì „ìã€‹ë¼ëŠ” ì´ë¦„ì´ì—ˆì§€ë§Œ ì–´ëŠ ìˆœê°„ 'ì‚¬ì´ì¢‹ì€'ì´ ë¹ ì§€ê³  ê·¸ëƒ¥ ã€Šê°€ìš°ìŠ¤ì „ìã€‹ê°€ ë˜ì—ˆë‹¤.\\n\\nê¸°ë³¸ì ìœ¼ë¡œã€ŠíŠ¸ë¼ìš°ë§ˆã€‹ì™€ ë¹„ìŠ·í•œ ë¶„ìœ„ê¸°ì˜ ê°œê·¸ë¬¼ì´ì§€ë§Œ ë¸”ë™ ì½”ë¯¸ë””ì˜ ì„±ê²©ì´ ê°•í•˜ë‹¤. ê²Œë‹¤ê°€ ì£¼ì œê°€ ì£¼ì œì¸ì§€ë¼ íšŒì‚¬ì› ì…ì¥ì—ì„œ ê³µê°í•  ë‚´ìš©ë“¤ì´ ë§ìœ¼ë©°, ê·¸ëŸ° ë…ìë“¤ì˜ ë°˜ì‘ì€ ëŒ€ê°œ 'ë‚´ê°€ ì›ƒì–´ë„ ì›ƒëŠ” ê²Œ ì•„ë‹ˆì•¼.' ë¡œ í†µì¼ë˜ì–´ ìˆë‹¤. ê·¸ë˜ë„ ë¶ˆí¸í•œ ì§„ì‹¤ê³¼ ë¸”ë™ ì½”ë¯¸ë”” ì‚¬ì´ì—ì„œ ë¸”ë™ ì½”ë¯¸ë”” ìª½ìœ¼ë¡œ ì¤„íƒ€ê¸°ë¥¼ ì˜ í•˜ê³  ìˆë‹¤. ë˜í•œ ë¸Œë¦¬í•‘ ì—í”¼ì†Œë“œë“¤ì„ ë³´ë©´ í˜„ ì—…ê³„ì˜ ì‹œë¥˜ë¥¼ ì˜ ì§‘ê³  ìˆì–´ ì‘ê°€ê°€ ì—í”¼ì†Œë“œ ì†Œì¬ì— ëŒ€í•œ ì¤€ë¹„ë¥¼ ì—´ì‹¬íˆ í•œ ë“¯í•œ ëŠë‚Œì´ ë‚œë‹¤. ê·¸ë ‡ê¸°ì— íŠ¹íˆ íšŒì‚¬ì›ë“¤, ì¦‰ ì–´ëŠ ì •ë„ ì—°ë ¹ì¸µì´ ë†’ì€ ë…ìë“¤ì—ê²Œ í‰ì´ ìƒë‹¹íˆ ì¢‹ë‹¤. ê·¸ë˜ë„ ìƒëŒ€ì ìœ¼ë¡œ ì•…ì—­ í¬ì§€ì…˜ì¸ ì¸ë¬¼ë“¤ì´ ìˆê³  ê·¸ë ‡ì§€ ì•Šì€ ì¸ë¬¼ë“¤ì´ ìˆì§€ë§Œ, í˜„ì‹¤ ê¸°ì—…ê³¼ëŠ” ë¹„êµë„ í•  ìˆ˜ ì—†ì„ë§Œí¼ì˜ í›ˆí›ˆí•¨ì´ íŠ¹ì§•.'''\n",
    "question = 'ã€Šê°€ìš°ìŠ¤ì „ìã€‹ì˜ ì›ë˜ ì œëª©ì€ ë¬´ì—‡ì´ì—ˆë‚˜?'\n",
    "generated_text = inference(question, context)\n",
    "print(f\"Generated text: {generated_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(context, question, answer):\n",
    "    trainer.model.eval()\n",
    "    # ì…ë ¥ í…ìŠ¤íŠ¸ë¥¼ í† í¬ë‚˜ì´ì¦ˆ\n",
    "    inputs = tokenizer(\n",
    "            text=question,\n",
    "            text_pair=context,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,    # contextë§Œ truncateí•˜ê¸°\n",
    "            max_length=384,\n",
    "            stride=128, # ì´ì „ chunkì™€ overlapë˜ëŠ” ë¶€ë¶„ì„ ë‘ì–´ ê¸´ ë¬¸ì„œë¥¼ ì²˜ë¦¬í•  ë•Œ ìœ ìš©. ëª¨ë¸ì´ ë” ë‚˜ì€ ì„ë² ë”©ì„ í•˜ë„ë¡ ë„ì™€ QAì— ìœ ìš©.\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "    del inputs['token_type_ids']\n",
    "\n",
    "    # GPU ì‚¬ìš© ì‹œ ì…ë ¥ì„ GPUë¡œ ì´ë™\n",
    "    inputs = {k: v.to('cuda') for k, v in inputs.items()}\n",
    "\n",
    "    outputs = model.generate(**inputs, max_new_tokens=30, num_beams=5)\n",
    "\n",
    "    # ê²°ê³¼ ì²˜ë¦¬\n",
    "    #predictions = outputs.argmax(-1)\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(f\"pred : {generated_text}\")\n",
    "    print(f\"answer : {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred : ë†’ì€ ë²½ë“¤ë¡œ ê°ì‹¼ í˜•íƒœ\n",
      "answer : {'answer_start': [789], 'text': [\"'ë‹¬ë¹› ì •ì›'(Moonlight Garden)\"]}\n",
      "pred : ë¯¸íƒ€ì¼€ ì„±\n",
      "answer : {'answer_start': [304], 'text': ['ë¯¸íƒ€ì¼€ì„±']}\n",
      "pred : ì´ˆë‚˜ë¼ì™•\n",
      "answer : {'answer_start': [1084], 'text': ['ì œ ì–‘ì™•']}\n",
      "pred : ìš°ì²œ\n",
      "answer : {'answer_start': [406], 'text': ['ìš°ì²œ']}\n",
      "pred : íƒœí™”ê´€\n",
      "answer : {'answer_start': [115], 'text': ['íƒœí™”ê´€(ì„œìš¸ì‹œ ì¢…ë¡œêµ¬ ì¸ì‚¬ë™ ì†Œì¬)']}\n",
      "pred : ê¹€ìˆ˜í™˜\n",
      "answer : {'answer_start': [0], 'text': ['ê¹€ìˆ˜í™˜ ì¶”ê¸°ê²½']}\n",
      "pred : ëŒ€ì§‘íšŒ\n",
      "answer : {'answer_start': [1029], 'text': ['ë°ì½”í–‰ì§„']}\n",
      "pred : ì˜ìƒëŒ€ì‚¬\n",
      "answer : {'answer_start': [35], 'text': ['ì˜ìƒëŒ€ì‚¬']}\n",
      "pred : ë¦¿ì§€ë‹¹\n",
      "answer : {'answer_start': [585], 'text': ['ë©”ì´ì € ë¦¿ì§€']}\n",
      "pred : ì–´ë¦¬ì„ê±°ë‚˜ ë‚˜ì˜ë‹¤ëŠ” ì‹\n",
      "answer : {'answer_start': [468], 'text': ['ê³µì •']}\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "datasets = load_from_disk('./resources/data/train_dataset')\n",
    "valid_dataset = datasets['validation']\n",
    "\n",
    "valid_dataset = valid_dataset.shuffle(seed=104)\n",
    "answers = [answer['text'] for answer in valid_dataset['answers']]\n",
    "test = Dataset.from_dict({'context':valid_dataset['context'][:10], 'question':valid_dataset['question'][:10], 'answers':valid_dataset['answers'][:10]})\n",
    "\n",
    "# pretrained model ê³¼ tokenizerë¥¼ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "\n",
    "assert model.config.vocab_size == len(tokenizer), \"Model and tokenizer vocab sizes do not match!\"\n",
    "for data in test:\n",
    "    inference(data['context'], data['question'], data['answers'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"KETI-AIR/ke-t5-small\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"KETI-AIR/ke-t5-small\", use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments\n",
    ")\n",
    "max_source_length = 384\n",
    "max_target_length = 16\n",
    "padding = \"max_length\"\n",
    "preprocessing_num_workers = 12\n",
    "num_beams = 5\n",
    "max_train_samples = 3952\n",
    "max_val_samples = 240\n",
    "num_train_epochs = 50\n",
    "train_batch_size = 16\n",
    "eval_batch_size = 8\n",
    "learning_rate = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(\n",
    "            tokenizer,\n",
    "            model=model,\n",
    "        )\n",
    "\n",
    "# Train preprocessing / ì „ì²˜ë¦¬ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤.\n",
    "def prepare_features(examples):\n",
    "    # truncationê³¼ padding(lengthê°€ ì§§ì„ë•Œë§Œ)ì„ í†µí•´ toknizationì„ ì§„í–‰í•˜ë©°, strideë¥¼ ì´ìš©í•˜ì—¬ overflowë¥¼ ìœ ì§€í•©ë‹ˆë‹¤.\n",
    "    # ê° exampleë“¤ì€ ì´ì „ì˜ contextì™€ ì¡°ê¸ˆì”© ê²¹ì¹˜ê²Œë©ë‹ˆë‹¤.\n",
    "    inputs = [f'question: {q}  context: {c}' for q, c in zip(examples['question'], examples['context'])]\n",
    "    targets = [f'{a[\"text\"][0]}' for a in examples['answers']]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_source_length, padding=padding, truncation=True, return_tensors='pt')\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=max_target_length, padding=padding, truncation=True, return_tensors='pt')\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    model_inputs[\"labels\"][model_inputs[\"labels\"] == tokenizer.pad_token_id] = -100\n",
    "        \n",
    "    return model_inputs\n",
    "\n",
    "train_dataset = train_dataset.map(\n",
    "            prepare_features,\n",
    "            batched=True,\n",
    "            num_proc=None,\n",
    "            remove_columns=train_dataset.column_names,\n",
    "            load_from_cache_file=not False)\n",
    "\n",
    "print('validation')\n",
    "val_dataset = val_dataset.map(\n",
    "            prepare_features,\n",
    "            batched=True,\n",
    "            num_proc=None,\n",
    "            remove_columns=val_dataset.column_names,\n",
    "            load_from_cache_file=not False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = load_metric('squad')\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [label.strip() for label in labels]\n",
    "\n",
    "    preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n",
    "    labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "\n",
    "    preds = np.where(preds != -100, preds, tokenizer.pad_token_id)\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    # decoded_labels is for rouge metric, not used for f1/em metric\n",
    "\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    formatted_predictions = [{\"id\": ex['id'], \"prediction_text\": decoded_preds[i]} for i, ex in enumerate(dataset['validation'].select(range(max_val_samples)))]\n",
    "    references = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in dataset['validation'].select(range(max_val_samples))]\n",
    "\n",
    "    result = metric.compute(predictions=formatted_predictions, references=references)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir='./ckpt/t5',\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    optim=\"adamw_hf\",\n",
    "    warmup_steps=1000,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=train_batch_size,\n",
    "    per_device_eval_batch_size=eval_batch_size,\n",
    "    predict_with_generate=True,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    save_strategy = 'epoch',\n",
    "    evaluation_strategy = 'epoch',\n",
    "    save_total_limit = 2,\n",
    "    logging_strategy = 'epoch',\n",
    "    logging_dir=\"./ckpt/t5/logs\",\n",
    "    load_best_model_at_end = True,\n",
    "    learning_rate=learning_rate,\n",
    "    remove_unused_columns = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience=10)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "trainer.train(resume_from_checkpoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(question, context):\n",
    "    inputs = f'question: {question}  context: {context} </s>'\n",
    "    sample = tokenizer(inputs, max_length=max_source_length, padding=padding, truncation=True, return_tensors='pt')\n",
    "    sample = sample.to(\"cuda:0\")\n",
    "    outputs = model.generate(**sample, max_length=max_target_length, num_beams=num_beams)\n",
    "    pred = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    pred = \"\\n\".join(nltk.sent_tokenize(pred))\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = ''''ã…‹ã…‹'ì€ ì¼ë°˜ì ìœ¼ë¡œ ì›ƒìŒì†Œë¦¬ë¥¼ í‘œí˜„í•  ë•Œ ì‚¬ìš©ëœë‹¤. í†µì‹ ì²´ì—ì„œëŠ” ì›ƒëŠ” ëª¨ìŠµì„ ì§ì ‘ì ìœ¼ë¡œ ë³´ì—¬ì¤„ ìˆ˜ ì—†ê¸° ë•Œë¬¸ì— ì˜ì„±ì–´ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš°ê°€ ë§ì€ë° ê·¸ ì¤‘ì—ì„œ 'í‚¥í‚¥', 'í­í­'ì„ ì´ˆì„±ì²´ë¡œ ë°”ê¾¸ì–´ ì‚¬ìš©í•œ ê²ƒì´ 'ã…‹ã…‹'ì— í•´ë‹¹í•œë‹¤. ì´ì— ëŒ€í•´ 2001ë…„ ì—°êµ¬ì—ì„œëŠ” \"ììŒë§Œ ê°€ì§€ê³  í‘œì‹œí•˜ì—¬ í†µì‹ ìƒì˜ ì¬ë¯¸ë¥¼ ë”í•˜ê¸° ìœ„í•œ ë°©ë²•ìœ¼ë¡œ ì‚¬ìš©ë˜ê³  ìˆë‹¤\"ê³  í•´ì„í•œ ë°” ìˆë‹¤. \\n\\nã…‹ìë¥¼ ì›ƒìŒì†Œë¦¬ë¥¼ ì˜ë¯¸í•˜ëŠ” ê²ƒìœ¼ë¡œ ì‚¬ìš©í•  ë•Œ, 'ã…‹'ì í•œ ê°œì—ì„œ 'ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹...'í•˜ëŠ” ì‹ìœ¼ë¡œ ì—´ íƒ€ ì´ìƒ í•œ ì¤„ ë„˜ê²Œê¹Œì§€ ì“°ì´ê¸°ë„ í•œë‹¤ ã…‹ìë¥¼ ì–¼ë§ˆë‚˜ ë§ì´ ì¼ëŠëƒì— ë”°ë¼ ê·¸ ëŠë‚Œê³¼ ì˜ë¯¸ëŠ” ë‹¬ë¼ì§„ë‹¤. ì‹ ì¡°ì–´ë¥¼ ì „ë¬¸ìœ¼ë¡œ ì—°êµ¬í•˜ëŠ” ì´ì¬í˜„ ë¬¸í™”í‰ë¡ ê°€ëŠ” ã€Ší•œêµ­ì¼ë³´ã€‹ì˜ ê¸°ê³ ê¸€ì—ì„œ ê·¸ ì¢…ë¥˜ì™€ ëŠë‚Œì´ ëŒ€ëµ ë‹¤ìŒê³¼ ê°™ë‹¤ê³  ë³´ì•˜ë‹¤\\n* ã…‹ - ì¼ë°˜ì ìœ¼ë¡œ ë¬´ì‹¬í•˜ê²Œ ë™ì˜í•˜ë ¤ëŠ” ìƒí™©ì—ì„œ ì“°ì¸ë‹¤. ë‹¤ë¥¸ ë§ëì— ë¶™ì˜€ì„ ë•Œ, ì´ë¥¼í…Œë©´ 'ê·¸ë˜ã…‹', 'ì¢‹ë„¤ã…‹', 'ì˜¬ã…‹' ë“±ì€ ì›ƒìŒ ìì²´ë¼ê¸°ë³´ë‹¤ëŠ” ì…ê¼¬ë¦¬ê°€ ì•„ì£¼ ì‚´ì§ ì˜¬ë¼ê°„ ìƒí™©ì´ë¼ëŠ” ëŠë‚Œì„ ì¤€ë‹¤. ì˜ì„±ì–´ë¡œëŠ” \"í­\" í˜¹ì€ \"í‚¥\"ìœ¼ë¡œ í•´ì„ë  ìˆ˜ ìˆë‹¤. í•œí¸ 'ë­í•´?ã…‹', 'ë¯¸ì•ˆã…‹'ì˜ ê²½ìš° 'ã…‹'ëŠ” ìƒíˆ¬ì ì¸ êµ°ë§ì˜ ì—­í• ì„ í•œë‹¤.\\n* ã…‹ã…‹ - ìœ„ì˜ ê²ƒë³´ë‹¤ ã…‹ìê°€ í•˜ë‚˜ê°€ ë” ë§ìœ¼ë‚˜ ì˜¤íˆë ¤ í˜•ì‹ì ì´ë¼ëŠ” ëŠë‚Œì„ ì¤€ë‹¤. ì¢‹ê²ŒëŠ” 'ê·¸ë ‡êµ°'ì„ ëœ»í•˜ê±°ë‚˜, ì¡°ê¸ˆ ë‚˜ì˜ê²ŒëŠ” ëŒ€í™”ìƒì—ì„œ ì˜ë¡€ì ìœ¼ë¡œ ì¶”ì„ìƒˆë¥¼ ë„£ê³  ìˆë‹¤ëŠ” ëŠë‚Œì„ ì¤€ë‹¤. 'ã…‹ã…‹'ì€ ë¯¸í˜¼ë‚¨ë…€ë“¤ì´ ê°€ì¥ ì‹«ì–´í•˜ëŠ” ì„±ì˜ ì—†ëŠ” ë©”ì‹ ì € ë§íˆ¬ë¡œ ì„ ì •ë˜ê¸°ë„ í–ˆë‹¤. ë‹¤ë§Œ íšŒì‚¬ ìƒì‚¬ì—ê²Œ ëŒ€ë‹µí•˜ëŠ” ê²½ìš° ê°™ì€ ê³µì ì¸ ìƒí™©ì—ì„œëŠ” 'ë„¤ã…‹ã…‹'ì™€ ê°™ì€ ëŒ€ë‹µì€ ê°€ë²¼ì›Œ ë³´ì¸ë‹¤ëŠ” ì¡°ì‚¬ë„ ìˆì—ˆë‹¤. \\n* ã…‹ã…‹ã…‹ - ë¹„êµì  ì¤‘ë¦½ì ì´ë©´ì„œë„ ê°€ì¥ ë§ì´ ì‚¬ìš©ë˜ëŠ” ìš©ë²•ì´ë‹¤. ì›ƒëŠ” ê°ì •ì„ ê·¸ëŒ€ë¡œ í‘œí˜„í•œë‹¤ëŠ” ë‰˜ì•™ìŠ¤ë¥¼ ê°–ëŠ”ë‹¤.\\n* ã…‹ã…‹ã…‹ã…‹... - ì—¬ê¸°ì„œë¶€í„°ëŠ” \"ì •ë§ ì›ƒê¸´ë‹¤\"ë¼ëŠ” ë°˜ì‘ì„ ë‚˜ë¦„ ì •ì„±ë“¤ì—¬ì„œ í‘œí˜„í•˜ëŠ” ëŠë‚Œì„ ì¤€ë‹¤. ë„¤ ê°œë¶€í„° ê·¸ ì´ìƒì€ ì˜ë¯¸ì˜ í•¨ì¶•ê³¼ ì •ì„œì˜ ê°•ë„ê°€ ê±°ì˜ ê°™ìœ¼ë©°, ì´ëŸ° ì ì—ì„œ 'ã…‹ã…‹ã…‹ã…‹'ëŠ” ì‹¤ì§ˆì ìœ¼ë¡œ ì›ƒê¸´ë‹¤ëŠ” ê²ƒì„ í‘œí˜„í•˜ëŠ” ìµœì†Œ ì¡°ê±´ì— í•´ë‹¹í•œë‹¤.'''\n",
    "question = 'ì›ƒëŠ” ê°ì •ì„ ê·¸ëŒ€ë¡œ í‘œí˜„í•œë‹¤ëŠ” ë‰˜ì•™ìŠ¤ë¥¼ ê°–ëŠ” ã…‹ì˜ ìš©ë²•ì€?'\n",
    "generated_text = generate_answer(question, context)\n",
    "print(f\"Generated text: {generated_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = '''ëƒ‰ëŒ€ ë™ê³„ ì†Œìš° ê¸°í›„(å†·å¸¦å†¬å­£å°‘é›¨æ°£å€™)ëŠ” ì¾¨íœì˜ ê¸°í›„ êµ¬ë¶„ì—ì„œ ëƒ‰ëŒ€ ê¸°í›„ì— ì†í•˜ë©°, ê¸°í˜¸ëŠ” Dwa, Dwb, Dwc, Dwdë¡œ, DëŠ” ëƒ‰ëŒ€, wëŠ” ë™ê³„ê±´ì¡°(wintertrocken)ì„ ê°€ë¦¬í‚¨ë‹¤. íŠ¸ë€ìŠ¤ ë°”ì´ì¹¼ ê¸°í›„ ë¼ê³ ë„ í•œë‹¤. ì—¬ë¦„ì—ëŠ” ì˜¨ë„ê°€ ë¹„êµì  ë†’ê³ , ê²¨ìš¸ì—ëŠ” ë§‘ì€ ë‚ ì”¨ê°€ ì§€ì†ë˜ê³  ì•½í•œ ë°”ëŒì´ ë¶„ë‹¤. ë³µì‚¬ëƒ‰ê° ë•Œë¬¸ì— ë§¤ìš° ìŒ€ìŒ€í•œ ê²ƒì´ íŠ¹ì§•ì´ë‹¤. ë˜ ì—¬ë¦„ì—ëŠ” ê³„ì ˆí’ì˜ ì˜í–¥ìœ¼ë¡œ ë¹„ê°€ ë§ì´ ë‚´ë¦°ë‹¤. ê²¨ìš¸ì€ ì—¬ë¦„ì— ë¹„í•´ ë§¤ìš° ê¸´ í¸ì´ë‹¤. ê¸°ì˜¨ì˜ ì—°êµì°¨ê°€ ë§¤ìš° í¬ê²Œ ë‚˜íƒ€ë‚˜ëŠ” ëŒ€ë¥™ì„± ê¸°í›„ê°€ ì§™ê²Œ ë‚˜íƒ€ë‚œë‹¤.\\n\\nëƒ‰ëŒ€ ë™ê³„ ì†Œìš° ê¸°í›„ëŠ” ë§¤ìš° ì¶”ìš´ ê¸°í›„ë¡œ, ë†ì‘ë¬¼ì„ ì¬ë°°í•  ìˆ˜ ìˆëŠ” ê³³ê³¼ ì¬ë°° ìˆ˜ì¢… ë° ìˆ˜ê°€ ì œí•œë˜ì–´ ìˆë‹¤. ìˆ˜ìˆ˜, ì˜¥ìˆ˜ìˆ˜, ë´„ë°€ ë“±ì„ ì¬ë°°í•œë‹¤. ë¶ë¶€ëŠ” íƒ€ì´ê°€ ê¸°í›„ì™€ ê°™ì´ íƒ€ì´ê°€ê°€ ë§ì´ ë‚˜íƒ€ë‚˜ ì„ì—…í™œë™ì„ í•œë‹¤. ìˆ² ë˜í•œ ëƒ‰ëŒ€ ìŠµìœ¤ ê¸°í›„ì™€ ë¹„ìŠ·í•˜ì—¬ ë‚¨ë¶€ì—ëŠ” ì¹¨ì—½ìˆ˜ì™€ í™œì—½ìˆ˜ê°€ í•¨ê»˜ ë‚˜íƒ€ë‚˜ëŠ” í˜¼í•©ë¦¼ì´, ë¶ë¶€ì—ëŠ” íƒ€ì´ê°€(ì¹¨ì—½ìˆ˜ë¦¼)ì´ ì˜ ë‚˜íƒ€ë‚œë‹¤. ë¶ë¶€ëŠ” ì¶”ìœ„ì—ë„ ì˜ ê²¬ë””ëŠ” ê°€ëŠ˜ê³  ë¾°ì¡±í•œ ìì„ ê°€ì§„ ì¹¨ì—½ìˆ˜ê°€ ìë¼ê¸° ìœ ë¦¬í•˜ê¸° ë•Œë¬¸ì— íƒ€ì´ê°€ê°€, ë‚¨ë¶€ì—ëŠ” ë¶ë¶€ë³´ë‹¤ ë”°ëœ»í•˜ì—¬ í˜¼í•©ë¦¼ì´ ì˜ ë‚˜íƒ€ë‚œë‹¤. ë‚¨ë¶€ ì§€ì—­ì˜ ê·¹ì†Œìˆ˜ì—ì„œë§Œ ë²¼ ì¬ë°°ê°€ ê°€ëŠ¥í•˜ë‹¤.\\n\\ní† ì–‘ì€ ì£¼ë¡œ í¬ë“œì¡¸ì´ ë‚˜íƒ€ë‚˜ê³ , í˜¼í•©ë¦¼ì—ì„œëŠ” ê°ˆìƒ‰ ì‚¼ë¦¼í† ê°€ ë‚˜íƒ€ë‚˜ëŠ”ë°, í¬ë“œì¡¸ì€ ìœ ê¸°ë¬¼ì´ ë¶„í•´ë˜ì§€ëŠ” ì•Šì•˜ìœ¼ë‚˜, ì†Œê¸ˆê¸°ë¥¼ ì˜ ë¨¸ê¸ˆì§€ ì•Šê³  ì¹¨ì—½ìˆ˜ë¦¼ì˜ íŠ¹ì„± ë•Œë¬¸ì— ë†ì‘ë¬¼ì„ ì¬ë°°í•˜ëŠ” ë°ì—ëŠ” ì í•©í•˜ì§€ ì•Šë‹¤. ê·¸ì— ë°˜í•´ ê°ˆìƒ‰ ì‚¼ë¦¼í† ëŠ” ë¬¼ì„ ì˜ í¡ìˆ˜í•˜ê³  í¬ë“œì¡¸ì— ë¹„í•´ ì•½ì‚°ì„±ì„ ë ê¸° ë•Œë¬¸ì— ë†ì‘ë¬¼ ì¬ë°°ì™€ ëª©ì¶•ì— ì•Œë§ë‹¤.\\n\\nì´ëŸ° ëƒ‰ëŒ€ ë™ê³„ ì†Œìš° ê¸°í›„ê°€ ë‚˜íƒ€ë‚˜ëŠ” ê³³ìœ¼ë¡œëŠ” ì‹œë² ë¦¬ì•„ ë‚´ë¥™ ë™ë¶€, ì¤‘êµ­ í™”ë² ì´ ë™ë¶€ ë° ë‘¥ë² ì´, ëŒ€í•œë¯¼êµ­ ê°•ì›ë„ ë‚´ë¥™ ì§€ì—­ê³¼ ê²½ê¸°ë„ ë° ì‚°ì•…ì§€ëŒ€, ì¡°ì„ ë¯¼ì£¼ì£¼ì˜ì¸ë¯¼ê³µí™”êµ­ ëŒ€ë¶€ë¶„, ìºë‚˜ë‹¤ ë° ë¯¸êµ­, ë„¤íŒ”ì˜ ì¼ë¶€ ì§€ì—­ ë“±ì´ ìˆìœ¼ë©°, íŠ¹íˆ ì‹œë² ë¦¬ì•„ ë™ë¶€ëŠ” ì„¸ê³„ì˜ í•œê·¹ì„ ë§Œë“¤ì–´ ë‚¸ë‹¤.\\n\\nê·¸ë˜ì„œ ì´ ê¸°í›„ê°€ ëƒ‰ëŒ€ ê¸°í›„ì˜ íƒˆì„ ì“´ ì‚¬ë°”ë‚˜ ê¸°í›„ë¼ê³  ë´ì•¼ í•˜ëŠ” ìˆ˜ì¤€ì— ì´ë¥´ê²Œ ë˜ë©°, ê²¨ìš¸ì² ì—ëŠ” ê°•ìˆ˜ëŸ‰ì´ ì ì–´ì ¸ì„œ ê·¸ë ‡ê²Œ ê±´ì¡°í•œ ë§Œí¼ ë¶ˆì”¨ê°€ ì‰½ê²Œ ì¼ì–´ë‚˜ëŠ” íŠ¹ì„±ìƒ ì‚°ë¶ˆë„ ë¬¼ë¡  ì¼ì–´ë‚˜ê¸°ê°€ ì‰¬ìš´ ê¸°í›„ ëŒ€ì—­ì— ì†í•œë‹¤.\\n\\nì•ìœ¼ë¡œ ì˜¨ë‚œí™” ì—¬íŒŒì— ë”°ë¼ ì´ ê¸°í›„ê°€ ì‚¬ë¼ì§ˆ ê°€ëŠ¥ì„±ì´ ìˆëŠ” ì§€ì—­ë„ ìƒê¸¸ ìˆ˜ë„ ìˆë‹¤. ë‹¨ì ì¸ ì˜ˆë¡œ ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ê¶Œ, ê°•ì›ë„ ë“±ì€ í˜„ì¬ Dwaë¡œ ë¶„ë¥˜ë˜ë‚˜ ì˜¨ëŒ€ ê¸°í›„ë¡œ ë°”ë€” ê°€ëŠ¥ì„±ì´ ìˆë‹¤.'''\n",
    "question = 'ëƒ‰ëŒ€ ë™ê³„ ì†Œìš° ê¸°í›„ì˜ wëŠ” ë¬´ì—‡ì˜ ì•½ìì¸ê°€?'\n",
    "generated_text = generate_answer(question, context)\n",
    "print(f\"Generated text: {generated_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = '''í•œêµ­ì˜ ì›¹íˆ°.\\n\\nê³½ë°±ìˆ˜ê°€ ë„¤ì´ë²„ì—ì„œ ì—°ì¬í•œ ì›¹íˆ°. íšŒì‚¬ ìƒí™œì˜ ì´ì•¼ê¸°ë¥¼ ì£¼ì¶•ìœ¼ë¡œ í•˜ëŠ” ì§ì¥ ë§Œí™”ë¡œ, ì‘ê°€ì˜ ì´ì „ í¥í–‰ì‘ì¸ã€ŠíŠ¸ë¼ìš°ë§ˆã€‹ì—ì„œë„ ìµíˆ ì´ë¦„ì„ ì•Œë ¸ë˜ ê°€ìš°ìŠ¤ ê·¸ë£¹#s-1.3ì´ ì•„ì˜ˆ ë©”ì¸ìœ¼ë¡œ ë‹¤ë¤„ì§„ë‹¤. ë”°ë¼ì„œ, ì–´ë–¤ ì˜ë¯¸ì—ì„œëŠ” íŠ¸ë¼ìš°ë§ˆì˜ ìŠ¤í•€ì˜¤í”„ì‘ì´ë¼ê³ ë„ ë³¼ìˆ˜ ìˆëŠ” ì…ˆ. ë¬¼ë¡ , ê°€ìš°ìŠ¤ ê·¸ë£¹ì˜ ë§‰ë‚˜ê°€ëŠ” ì„¼ìŠ¤ëŠ” ì—¬ì „í•˜ë‹¤. ì´ˆê¸°ì—ëŠ” ã€Šì‚¬ì´ì¢‹ì€ ê°€ìš°ìŠ¤ì „ìã€‹ë¼ëŠ” ì´ë¦„ì´ì—ˆì§€ë§Œ ì–´ëŠ ìˆœê°„ 'ì‚¬ì´ì¢‹ì€'ì´ ë¹ ì§€ê³  ê·¸ëƒ¥ ã€Šê°€ìš°ìŠ¤ì „ìã€‹ê°€ ë˜ì—ˆë‹¤.\\n\\nê¸°ë³¸ì ìœ¼ë¡œã€ŠíŠ¸ë¼ìš°ë§ˆã€‹ì™€ ë¹„ìŠ·í•œ ë¶„ìœ„ê¸°ì˜ ê°œê·¸ë¬¼ì´ì§€ë§Œ ë¸”ë™ ì½”ë¯¸ë””ì˜ ì„±ê²©ì´ ê°•í•˜ë‹¤. ê²Œë‹¤ê°€ ì£¼ì œê°€ ì£¼ì œì¸ì§€ë¼ íšŒì‚¬ì› ì…ì¥ì—ì„œ ê³µê°í•  ë‚´ìš©ë“¤ì´ ë§ìœ¼ë©°, ê·¸ëŸ° ë…ìë“¤ì˜ ë°˜ì‘ì€ ëŒ€ê°œ 'ë‚´ê°€ ì›ƒì–´ë„ ì›ƒëŠ” ê²Œ ì•„ë‹ˆì•¼.' ë¡œ í†µì¼ë˜ì–´ ìˆë‹¤. ê·¸ë˜ë„ ë¶ˆí¸í•œ ì§„ì‹¤ê³¼ ë¸”ë™ ì½”ë¯¸ë”” ì‚¬ì´ì—ì„œ ë¸”ë™ ì½”ë¯¸ë”” ìª½ìœ¼ë¡œ ì¤„íƒ€ê¸°ë¥¼ ì˜ í•˜ê³  ìˆë‹¤. ë˜í•œ ë¸Œë¦¬í•‘ ì—í”¼ì†Œë“œë“¤ì„ ë³´ë©´ í˜„ ì—…ê³„ì˜ ì‹œë¥˜ë¥¼ ì˜ ì§‘ê³  ìˆì–´ ì‘ê°€ê°€ ì—í”¼ì†Œë“œ ì†Œì¬ì— ëŒ€í•œ ì¤€ë¹„ë¥¼ ì—´ì‹¬íˆ í•œ ë“¯í•œ ëŠë‚Œì´ ë‚œë‹¤. ê·¸ë ‡ê¸°ì— íŠ¹íˆ íšŒì‚¬ì›ë“¤, ì¦‰ ì–´ëŠ ì •ë„ ì—°ë ¹ì¸µì´ ë†’ì€ ë…ìë“¤ì—ê²Œ í‰ì´ ìƒë‹¹íˆ ì¢‹ë‹¤. ê·¸ë˜ë„ ìƒëŒ€ì ìœ¼ë¡œ ì•…ì—­ í¬ì§€ì…˜ì¸ ì¸ë¬¼ë“¤ì´ ìˆê³  ê·¸ë ‡ì§€ ì•Šì€ ì¸ë¬¼ë“¤ì´ ìˆì§€ë§Œ, í˜„ì‹¤ ê¸°ì—…ê³¼ëŠ” ë¹„êµë„ í•  ìˆ˜ ì—†ì„ë§Œí¼ì˜ í›ˆí›ˆí•¨ì´ íŠ¹ì§•.'''\n",
    "question = 'ã€Šê°€ìš°ìŠ¤ì „ìã€‹ì˜ ì›ë˜ ì œëª©ì€ ë¬´ì—‡ì´ì—ˆë‚˜?'\n",
    "generated_text = generate_answer(question, context)\n",
    "print(f\"Generated text: {generated_text}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
